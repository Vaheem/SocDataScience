{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Classification with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 1: No Free Lunch\n",
    "\n",
    "In this task we consider the Breast Cancer Wisconsin dataset which is included in the ```sklearn.datasets``` module. In this dataset we aim to predict/classify whether a tumor is malignant or benign, using features of a mammal screen image. More information on the dataset can be found below, feel free to further explore the dataset yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "dta = datasets.load_breast_cancer()\n",
    "print(dta.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dta.data\n",
    "y = dta.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to apply three different classifiers in sklearn, namely a logistic regression classifier ```LogisticRegression```, a support vector classifier ```SVC```, and a Nearest Neighbors classifier ```KNeighborsClassifier```, to perform classification on this dataset. The corresponding algorithms are imported in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Applying Default Settings\n",
    "\n",
    "Split the data into a training and a test set, using a relative test set size of 30%. Afterwards, apply each of the three algorithms with their default parameters on the test data and check their accuracy and AUC score. Which algorithm appears to work best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649122807017544\n",
      "0.9777943368107302\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train,y_train)\n",
    "y_pred1 = svm.predict(X_test)\n",
    "y_score1 = svm.decision_function(X_test)\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print(roc_auc_score(y_test, y_score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771\n",
      "0.9918032786885246\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(max_iter = 10000)\n",
    "logit.fit(X_train,y_train)\n",
    "y_pred2 = logit.predict(X_test)\n",
    "y_score2 = logit.predict_proba(X_test)[:,1]\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "print(roc_auc_score(y_test, y_score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9473684210526315\n",
      "0.9749627421758569\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred3 = knn.predict(X_test)\n",
    "y_score3 = knn.predict_proba(X_test)[:,1]\n",
    "print(accuracy_score(y_test, y_pred3))\n",
    "print(roc_auc_score(y_test, y_score3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ This dataset seems rather easy to classify, but Logistic Regression seems to have the edge over the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Tweaking Parameters\n",
    "\n",
    "We now want to check whether we can even improve the performances of our algorithms. For that matter, we want to tweak some parameters of the three given algorithms. More precisely, we want to tweak the following parameters:\n",
    "* ```LogisticRegression```: the regularization parameter ```C``` and the ```penalty``` function\n",
    "* ```SVC```: the ```kernel```function and the regularization parameter ```C```\n",
    "* ```KNeighborsClassifier```: the neighborhood size ```n_neighbors```\n",
    "\n",
    "Conduct your own parameter search and determine the parameter setting which yields the best cross-validated accuracy score. Is there a clear best algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = dict({'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                   'C': [0.01,0.1,1,10,100,1000,10000]})\n",
    "\n",
    "svm = SVC()\n",
    "clf1 = GridSearchCV(svm, svm_params)\n",
    "clf1.fit(X,y)\n",
    "\n",
    "res1 = clf1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([1.28157616e-02, 3.20281982e-03, 6.80594444e-03, 7.20634460e-03,\n",
      "       9.66874599e-02, 2.40182877e-03, 4.00347710e-03, 7.40656853e-03,\n",
      "       8.10936642e-01, 2.00166702e-03, 3.00240517e-03, 7.00616837e-03,\n",
      "       2.41839724e+00, 2.40173340e-03, 2.20150948e-03, 6.40554428e-03,\n",
      "       6.00745931e+00, 3.20267677e-03, 2.60152817e-03, 5.60460091e-03,\n",
      "       6.06050692e+00, 9.60860252e-03, 3.80344391e-03, 5.20462990e-03,\n",
      "       6.80478363e+00, 3.28297615e-02, 1.00095272e-02, 5.20467758e-03]), 'std_fit_time': array([1.16801907e-03, 4.00352535e-04, 4.00567094e-04, 4.00590911e-04,\n",
      "       7.14611898e-02, 4.90465972e-04, 2.33601546e-07, 4.90388050e-04,\n",
      "       3.08666158e-01, 2.86102295e-07, 3.01578299e-07, 6.33239046e-04,\n",
      "       4.93320444e-01, 4.90641167e-04, 4.00328732e-04, 4.90213029e-04,\n",
      "       1.72759707e+00, 4.00424128e-04, 4.90582726e-04, 2.33415391e-03,\n",
      "       4.76947603e+00, 2.94209467e-03, 7.48888953e-04, 1.60155297e-03,\n",
      "       4.04330693e+00, 6.24677996e-03, 2.28252760e-03, 1.60133844e-03]), 'mean_score_time': array([0.00020022, 0.00060058, 0.00400372, 0.00200191, 0.00060086,\n",
      "       0.00040054, 0.00200181, 0.00180182, 0.00040073, 0.00080085,\n",
      "       0.00140152, 0.00180178, 0.00060086, 0.00060096, 0.00160165,\n",
      "       0.00140171, 0.00020037, 0.00020027, 0.0010015 , 0.00100136,\n",
      "       0.00040054, 0.00020022, 0.00100093, 0.00160155, 0.00080123,\n",
      "       0.00060058, 0.00040026, 0.00160151]), 'std_score_time': array([4.00447845e-04, 4.90368586e-04, 1.78416128e-07, 2.86102295e-07,\n",
      "       4.90602233e-04, 4.90563246e-04, 9.53674316e-08, 4.00352506e-04,\n",
      "       4.90796870e-04, 4.00424156e-04, 4.90193483e-04, 4.00090313e-04,\n",
      "       4.90602280e-04, 4.90680070e-04, 4.90135170e-04, 4.90232463e-04,\n",
      "       4.00733948e-04, 4.00543213e-04, 2.86102295e-07, 5.00111031e-07,\n",
      "       4.90563269e-04, 4.00447845e-04, 2.33601546e-07, 8.00776501e-04,\n",
      "       4.00614976e-04, 4.90368794e-04, 4.90212867e-04, 8.00752732e-04]), 'param_C': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1,\n",
      "                   10, 10, 10, 10, 100, 100, 100, 100, 1000, 1000, 1000,\n",
      "                   1000, 10000, 10000, 10000, 10000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['linear', 'poly', 'rbf', 'sigmoid', 'linear', 'poly',\n",
      "                   'rbf', 'sigmoid', 'linear', 'poly', 'rbf', 'sigmoid',\n",
      "                   'linear', 'poly', 'rbf', 'sigmoid', 'linear', 'poly',\n",
      "                   'rbf', 'sigmoid', 'linear', 'poly', 'rbf', 'sigmoid',\n",
      "                   'linear', 'poly', 'rbf', 'sigmoid'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01, 'kernel': 'linear'}, {'C': 0.01, 'kernel': 'poly'}, {'C': 0.01, 'kernel': 'rbf'}, {'C': 0.01, 'kernel': 'sigmoid'}, {'C': 0.1, 'kernel': 'linear'}, {'C': 0.1, 'kernel': 'poly'}, {'C': 0.1, 'kernel': 'rbf'}, {'C': 0.1, 'kernel': 'sigmoid'}, {'C': 1, 'kernel': 'linear'}, {'C': 1, 'kernel': 'poly'}, {'C': 1, 'kernel': 'rbf'}, {'C': 1, 'kernel': 'sigmoid'}, {'C': 10, 'kernel': 'linear'}, {'C': 10, 'kernel': 'poly'}, {'C': 10, 'kernel': 'rbf'}, {'C': 10, 'kernel': 'sigmoid'}, {'C': 100, 'kernel': 'linear'}, {'C': 100, 'kernel': 'poly'}, {'C': 100, 'kernel': 'rbf'}, {'C': 100, 'kernel': 'sigmoid'}, {'C': 1000, 'kernel': 'linear'}, {'C': 1000, 'kernel': 'poly'}, {'C': 1000, 'kernel': 'rbf'}, {'C': 1000, 'kernel': 'sigmoid'}, {'C': 10000, 'kernel': 'linear'}, {'C': 10000, 'kernel': 'poly'}, {'C': 10000, 'kernel': 'rbf'}, {'C': 10000, 'kernel': 'sigmoid'}], 'split0_test_score': array([0.92105263, 0.80701754, 0.75438596, 0.62280702, 0.93859649,\n",
      "       0.83333333, 0.83333333, 0.62280702, 0.94736842, 0.84210526,\n",
      "       0.85087719, 0.54385965, 0.93859649, 0.88596491, 0.87719298,\n",
      "       0.48245614, 0.93859649, 0.87719298, 0.89473684, 0.48245614,\n",
      "       0.94736842, 0.87719298, 0.9122807 , 0.47368421, 0.93859649,\n",
      "       0.92105263, 0.94736842, 0.47368421]), 'split1_test_score': array([0.94736842, 0.83333333, 0.77192982, 0.62280702, 0.94736842,\n",
      "       0.85087719, 0.86842105, 0.62280702, 0.92982456, 0.88596491,\n",
      "       0.89473684, 0.45614035, 0.93859649, 0.92105263, 0.92105263,\n",
      "       0.40350877, 0.94736842, 0.92982456, 0.93859649, 0.39473684,\n",
      "       0.94736842, 0.94736842, 0.93859649, 0.39473684, 0.94736842,\n",
      "       0.94736842, 0.93859649, 0.39473684]), 'split2_test_score': array([0.99122807, 0.85964912, 0.78070175, 0.63157895, 0.98245614,\n",
      "       0.89473684, 0.90350877, 0.63157895, 0.97368421, 0.92982456,\n",
      "       0.92982456, 0.46491228, 0.97368421, 0.90350877, 0.9122807 ,\n",
      "       0.42105263, 0.97368421, 0.92982456, 0.92982456, 0.40350877,\n",
      "       0.97368421, 0.95614035, 0.98245614, 0.40350877, 0.97368421,\n",
      "       0.98245614, 0.98245614, 0.40350877]), 'split3_test_score': array([0.92105263, 0.88596491, 0.79824561, 0.63157895, 0.92105263,\n",
      "       0.93859649, 0.93859649, 0.63157895, 0.92105263, 0.94736842,\n",
      "       0.94736842, 0.38596491, 0.94736842, 0.93859649, 0.95614035,\n",
      "       0.34210526, 0.97368421, 0.94736842, 0.94736842, 0.34210526,\n",
      "       0.92982456, 0.92982456, 0.93859649, 0.34210526, 0.97368421,\n",
      "       0.94736842, 0.93859649, 0.34210526]), 'split4_test_score': array([0.95575221, 0.88495575, 0.84070796, 0.62831858, 0.95575221,\n",
      "       0.91150442, 0.91150442, 0.62831858, 0.95575221, 0.9380531 ,\n",
      "       0.9380531 , 0.45132743, 0.96460177, 0.95575221, 0.94690265,\n",
      "       0.36283186, 0.98230088, 0.9380531 , 0.94690265, 0.87610619,\n",
      "       0.97345133, 0.9380531 , 0.96460177, 0.86725664, 0.97345133,\n",
      "       0.95575221, 0.97345133, 0.85840708]), 'mean_test_score': array([0.94729079, 0.85418413, 0.78919422, 0.6274181 , 0.94904518,\n",
      "       0.88580966, 0.89107281, 0.6274181 , 0.94553641, 0.90866325,\n",
      "       0.91217202, 0.46044093, 0.95256948, 0.920975  , 0.92271386,\n",
      "       0.40239093, 0.96312684, 0.92445272, 0.93148579, 0.49978264,\n",
      "       0.95433939, 0.92971588, 0.94730632, 0.49625834, 0.96135693,\n",
      "       0.95079957, 0.95609377, 0.49448843]), 'std_test_score': array([0.02599603, 0.0304835 , 0.02938499, 0.00394868, 0.02028224,\n",
      "       0.03873088, 0.03654464, 0.00394868, 0.01868869, 0.03938232,\n",
      "       0.03544367, 0.05025317, 0.01419963, 0.02470123, 0.02787898,\n",
      "       0.0489065 , 0.01697425, 0.02450019, 0.01946147, 0.19343477,\n",
      "       0.01695673, 0.02770574, 0.02413777, 0.19016919, 0.01525722,\n",
      "       0.01967279, 0.01835593, 0.18671792]), 'rank_test_score': array([ 9, 20, 21, 22,  7, 19, 18, 22, 10, 17, 16, 27,  5, 15, 14, 28,  1,\n",
      "       13, 11, 24,  4, 12,  8, 25,  2,  6,  3, 26])}\n"
     ]
    }
   ],
   "source": [
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=100, kernel='linear')\n",
      "0.9631268436578171\n"
     ]
    }
   ],
   "source": [
    "print(clf1.best_estimator_)\n",
    "print(clf1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n",
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n"
     ]
    }
   ],
   "source": [
    "logit_params = dict({'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                   'C': [0.0001,0.001,0.01,0.1,1,10,100]})\n",
    "\n",
    "logit = LogisticRegression(solver = 'saga', max_iter = 10000, l1_ratio = 0.5)\n",
    "clf2 = GridSearchCV(logit, logit_params)\n",
    "clf2.fit(X,y)\n",
    "\n",
    "res2 = clf2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, l1_ratio=0.5, max_iter=10000, solver='saga')\n",
      "0.9226672876882471\n"
     ]
    }
   ],
   "source": [
    "print(clf2.best_estimator_)\n",
    "print(clf2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = dict({'n_neighbors': [1,3,5,10,20,30,50,100]})\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "clf3 = GridSearchCV(knn, knn_params)\n",
    "clf3.fit(X,y)\n",
    "\n",
    "res3 = clf3.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=20)\n",
      "0.9314857941313461\n"
     ]
    }
   ],
   "source": [
    "print(clf3.best_estimator_)\n",
    "print(clf3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ On the given data, a linear SVM with C = 100 seems to yield the best cross-validated accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Multi-Class Classification\n",
    "\n",
    "In lecture we have focused on the binary case in classification. However, in many applications there are more than two classes to predict.  \n",
    "One standard strategy to extend binary classification methods to the multiclass case is the so-called _one-vs-all_ approach, which requires a classifier that can also produce a confidence score for its predictions rather than the crisp predictions themselves. In logistic regression for instance, such a confidence score is given by the class probabilities.  \n",
    "Assuming that you there are $L$ classes $C_1,\\dots, C_L$, in the one-vs-all approach one fits $L$ binary models where in the $l$-th model the goal is to predict whether a sample belongs to class $C_l$ or not. After fitting the $l$-th model, the confidence score of each sample belonging to class $C_l$ is then computed and stored, and in the end, the class that achieved the highest confidence score among all $L$ classes is then chosen as the final prediction.\n",
    "\n",
    "In this task, we are going to implement this approach and apply it on a student evaluation dataset, aiming to predict which of three course instructors has given a class. The dataset is loaded in the cell below, and documented on http://archive.ics.uci.edu/ml/datasets/turkiye+student+evaluation. Feel free to explore the data beforehand. Note that we drop the class (lecture) identifier, since this uniquely determines the instructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instr</th>\n",
       "      <th>attendance</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>...</th>\n",
       "      <th>Q19</th>\n",
       "      <th>Q20</th>\n",
       "      <th>Q21</th>\n",
       "      <th>Q22</th>\n",
       "      <th>Q23</th>\n",
       "      <th>Q24</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q26</th>\n",
       "      <th>Q27</th>\n",
       "      <th>Q28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   instr  attendance  difficulty  Q1  Q2  Q3  Q4  Q5  Q6  Q7  ...  Q19  Q20  \\\n",
       "0      1           0           4   3   3   3   3   3   3   3  ...    3    3   \n",
       "1      1           1           3   3   3   3   3   3   3   3  ...    3    3   \n",
       "2      1           2           4   5   5   5   5   5   5   5  ...    5    5   \n",
       "3      1           1           3   3   3   3   3   3   3   3  ...    3    3   \n",
       "4      1           0           1   1   1   1   1   1   1   1  ...    1    1   \n",
       "\n",
       "   Q21  Q22  Q23  Q24  Q25  Q26  Q27  Q28  \n",
       "0    3    3    3    3    3    3    3    3  \n",
       "1    3    3    3    3    3    3    3    3  \n",
       "2    5    5    5    5    5    5    5    5  \n",
       "3    3    3    3    3    3    3    3    3  \n",
       "4    1    1    1    1    1    1    1    1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00262/turkiye-student-evaluation_generic.csv\"\n",
    "\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00262/turkiye-student-evaluation_generic.csv\",\n",
    "                  index_col = False,\n",
    "                  sep = ',',\n",
    "                  skipinitialspace = True)\n",
    "\n",
    "# drop class for training purposes - this would uniquely identify instructors\n",
    "df = df.drop(columns = ['class','nb.repeat'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Implementing Multiclass-Classification with Logistic Regression\n",
    "\n",
    "Implement a function mc_predict() that uses the one-vs-all approach to perform multiclass classififcation based on logistic regression. Thus, you have to fit multiple binary models. Use the function signature given in the cell below, i.e. include as input both training data as well as a test set to predict on after fitting the model. You may use sklearn to fit the binary models.  \n",
    "Apply your function on the student evaluation data, and compute the accuracy on both training and test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def mc_predict(X_train,y_train,X_test):\n",
    "    \"\"\"\n",
    "    :param X_train: two dimensional numpy array describing the feature matrix to train on\n",
    "    :param y_train: one dimensional numpy array or list representing the class vector used in training\n",
    "    :param X_test: two dimensional numpy array describing the feature matrix to train on.\n",
    "    :\n",
    "    :return: one dimensional numpy array epresenting the predictions on test data\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = np.unique(y_train)\n",
    "    M = len(Y)\n",
    "    \n",
    "    # collect class-wise confidence scores in this matrix\n",
    "    y_probs = np.zeros((X_test.shape[0],M))\n",
    "    # iterate over all classes\n",
    "    for i in range(M):\n",
    "        # fit binary model\n",
    "        y_curr = np.array(y_train==Y[i]).astype(int) \n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X_train, y_curr)\n",
    "        y_probs[:,i] = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # return classes that yielded highest confidence\n",
    "    return np.array([Y[i] for i in np.argmax(y_probs, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.621441395229546\n",
      "0.6184279021343051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:,1:].to_numpy()\n",
    "y = df.iloc[:,0].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 5)\n",
    "\n",
    "\n",
    "y_pred = mc_predict(X_train,y_train,X_train)\n",
    "print(accuracy_score(y_train,y_pred))\n",
    "\n",
    "y_pred = mc_predict(X_train,y_train,X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Comparison to Scikit-learn\n",
    "\n",
    "Note that the logistic regreesion in sklearn also enables the multiclass case as it has a built-in one-vs-all functionality. Compare your predictions and accuracy scores to those resulting from the built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62426263144396\n",
      "0.6173867777199376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train,y_train))\n",
    "print(clf.score(X_test,y_test)) # same results as in our implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: The Ensemble Effect: Decision Trees and Random Forests\n",
    "\n",
    "Other than logistic regressors, decision trees (and thus also random forests) naturally allow for multiclass classification without having to employ a one-vs-all strategy.\n",
    "\n",
    "#### a) Classification with Decision Trees\n",
    "\n",
    "Apply the decision tree classifier from sklearn on the student evaluation data and compare the accuracies on training and test data with those resulting from the logistic regression. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7966145165427032\n",
      "0.5455491931285789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "print(clf.score(X_train,y_train))\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The decision tree model appears to work very well on the training data, but not to generalize that well. In particular, a single tree by itself gets outperformed by the logistic regression models that we used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Growing a Forest\n",
    "\n",
    "For all $n\\in\\{3,\\dots,100\\}$, apply sklearns functionalities to fit a random forest with $n$ trees on the student evaluation data. For each model, compute the accuracy on the test set, and plot the number of trees against the resulting accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "s=[]\n",
    "for n in np.arange(3,101):\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n)\n",
    "    clf.fit(X_train, y_train)\n",
    "    s.append(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdJklEQVR4nO3df+wc9X3n8eeLL1/Kl1zbb1JMqb/8ciXHTQgCkq+cUF8jCuXsEBo4DilOFKmqeudzVK4J6vlk1LukF+kUn6gu4QIp8hF6zbUFcsRxrEJwonJNENcQf41NjDFOXGjw9+s0GBJDge8F23nfHztr1svO7uzu7O7szOshWd6dmd35fGbWb3/mPZ/PZxQRmJlZeZ0y6gKYmdlgOdCbmZWcA72ZWck50JuZlZwDvZlZyTnQm5mVXKZAL2mNpP2SDkjamLLN5ZJ2S9or6ZsNy++S9JykJ/IqtJmZZadO/eglTQDfA64C5oEdwIci4smGbaaB/wusiYhnJZ0VEc8l694LvAx8MSLeMZBamJlZqlMzbLMSOBARTwNIuge4FniyYZsPA1si4lmAepBPXn9L0gXdFOrMM8+MCy7o6iNmZpW2c+fO5yNiSat1WQL9DHCw4f088O6mbd4KTEr6W+DngVsj4ovdFFLSOmAdwHnnncfc3Fw3HzczqzRJP0hblyVHrxbLmvM9pwLvAt4PrAb+k6S3Zi4hEBGbI2I2ImaXLGn5n5KZmfUgS4t+Hji34f05wKEW2zwfEa8Ar0j6FnAxtdy+mZmNUJYW/Q5guaRlkk4D1gLbmrb5KvAbkk6VdAa11M6+fItqZma96BjoI+IYcCOwnVrw/lJE7JW0XtL6ZJt9wIPAd4HvAHdGxBMAku4G/g5YIWle0u8NpipmZtZKx+6VozA7Oxu+GWtmlp2knREx22pdlhy9mXVh664Fbtm+n0NHFlk6PcWG1Su47tKZsduHlYcDvVmOtu5a4OYte1g8ehyAhSOL3LxlD0BugXgY+7By8Vw3Zjm6Zfv+EwG4bvHocW7Zvn+s9mHl4kBvlqNDRxa7Wl7UfVi5OHVjlTLo3PbS6SkWWgTcpdNTY7WPIkg7V74/0T236K0y6rnthSOLBK/ntrfuWshtHxtWr2BqcuKkZVOTE2xYvWKs9jFqaefqP27dM/BzWEZu0VtltMtt59UirH/PIFucve5jnHoDpZ2rux89yPGmLuGdzqGvABzorUKGldu+7tKZgQeSbvcxbr2B0s5Jc5DvtL17KNU4dWOVkZbDLltuu5Vx6w3U7TlJ274IPZS27lpg1aaHWLbxflZtemgkaSYHequMKuS204xbb6BW5ypNu3M46h5Kw7gvlIUDvVXGdZfO8OnrL2JmegoB01OTnD55Cjfdu3tkLa1hGcbVTJ77aDxX7cxMT/Hp6y9KTcOM6iqu3or/+L27R35FAQ70VjHXXTrDIxuv4DMfvISfHvsZP3n1aCV6b4xjb6D6uWr1QAyoPSjjkY1XtM2191qmftItja34NMMe8+BAb5VUhNztMDVfzXRqCRdpH/20ynspU7/plla/rV7Knif3urFK6jV3O8pBPP3uo4i9gbLYsHrFST1noLsrhW7L1G833E6/oVHcF3Kgt0rqZXRpWle9uR/8mC/vXBibrovjZhhjExr1ewM37bcFtSuKUfTjd6C3UurU+u2llZjnIJ5udUo1DWLwVJEGGg3jaqSu3ykm0n5bn77+IqB2rm66d/dQj6kDvZVOltZvL63EvAbx9CLtu+p166aln+X4VPkKIo9UEbzxtwWM7Jj6CVNWOqs2PdSyRTYzPcUjG6/I/XsnpJbBvn6ZnkeruJd9p9U1y/EZ1DGEwV0p5Pm9gyjjII8p+AlTVjGDGiST1tL7V++aOSlHX1/+m7+2JLcWXNq+03p3tKtrluMzqGM4qCuFvL93EKmiUQ7ecqCvoGHnXhv394tTk0hw5NWjPe07S9kHNY1vu3TP7PlvecPybnpvZD1GrfbRbV3Tjk9Qa3VuWL1iYMdwUBPLDWPCun5lOe6DKqsDfcUMO/favL8ji0dPrOt231nL3m+OtZ20ll6r5Tfdu7vldzS34Lo5Rq323W1dWx2f5v2lXaX0ewwH1aod9VQHWWQ57jCYf4cO9CXWqvXba++NXq8COg0e6abVlbXVNuzueGmytoqzHKOP37ubW7bvf0M9eqlr42dalW/x6HH+4tvPnpgioterr1YGdaUwDg9jyXLcB3UF4puxJdXcSoT2Od1W6xu7hKV1F+v0o1y28X46/cIEPLPp/R22Sv+urJ8ftrRz0Hzcshyjdp/vR6d9572/rMekKN87KIP4LftmbAWltX7TemnU1ze/r7f0e81/ths80rhNFsNqteV1TyFrazvLMarrptXXz/2M5v1lqUc3+8/7aiuP7x3mvathX4G4RV9S7VpqgswtyPqEUr22Plq1tBp10+oaZKut/o984chi2+MziFZip2PUrNfj3qrsWfeddrWXNv0DvPEqsH5cRzU6tC5reZsHOeX5H0Cr497v8XGLvoLatdSC7MG+3sLotfXR3NIaRgu5W83/6Nodl0HkUdOO0U9ePdpy+yzHvZf7GWm/lwmp7dVeqxvkp0+e8obP1I/rKAdfpd3Qb1XexaPH+eNte/npsZ/l3nmh+bg3/nscxPFxi75g8rp87LaV2EoeOfpxkDaQJc2w7gn0cwXTSw642/s6oru0U7PmgULDSJ10e67T5DXICfIbSNWuRZ9pmmJJayTtl3RA0saUbS6XtFvSXknf7OazVpPn02iyPrghTeN0rsOY4naUuu2CN6yeHP0c916m9k3bX9pvaOn0VF/dFxs/O6wnMeXV3bKoT+ZK0zF1I2kCuB24CpgHdkjaFhFPNmwzDXweWBMRz0o6K+tn7XV5D/qoB+m0FsP01ORJl6WQ3mLMMlJwlAOx+tlfN63SfvqS91LeXkdo9jqWoNu++mkpn1a/rWaN/+kMa8BTt+f69MlTWqbQmv/D7Oe3OIwbs1la9CuBAxHxdES8BtwDXNu0zYeBLRHxLEBEPNfFZy0xyKH7rZ6y88cfuDC3lvqwn42Z5/5aHZ/6TejpqUnefMbk2B2fPK/C2n1Xlt8W8IanRDX/pzOsAU9Zn0Vbr+Mnf/vCjk+o6vfcDuPpX1luxs4ABxvezwPvbtrmrcCkpL8Ffh64NSK+mPGzAEhaB6wDOO+887KUvXRGMXS/cX0/hj0EPW1/aQOL2hnGAKtRDNHPc76WdiOCofNvq1OLd1jdDbPcfG6VG2++UX5Tw++s33M7jN9flkDf6pGNzfd5TgXeBVwJTAF/J+nbGT9bWxixGdgMtZuxGcpVOqMYup+XYQ9Bb/e9vfRaKNvxGaYsx67TNoP87aeVJe3mc/M+07ZvniK6WTfndtC/vyyBfh44t+H9OcChFts8HxGvAK9I+hZwccbPjqVB5KOLMnS/F3m3yHptAdY1tqiK8ACNcRiiP0p5//aznPNu95nWck9TpHPbsXulpFOB71FrrS8AO4APR8Tehm3eBtwGrAZOA74DrAWe6vTZVorevXLchlsPQ57HJMt3Zek+KuAzH7ykEOfKv5nhGdSxHuVUFVn01b0yIo4BNwLbgX3AlyJir6T1ktYn2+wDHgS+Sy3I3xkRT6R9No9KjVKnicGqKM+bf1mOb5buo0unpwpzrpqPT33CsJvu3c2qTQ/ldlN2664FVm16iGUb78/1e4uqVX0Hdc6zttCL2P3YA6Z6MG6Ta42bbo9vuxbcTffuLty58sRe+ehlgFc/5zzrVeSofld9D5iyk/UyGMWy6/b4truaKOK5GlSLsyhXL8PSbuK+VvLovZblKrKIHOh7MIx+r1XWy/G97tIZHtl4Bc9sej+PbLziRAu2iOeqyg/fyFO7h7UP6pzXf2ef/eAlhftdteNA34NRTwlQ9jxsnvnsUZ+rVgZ1lVHEq5dBSqtX49QNgzrnRfxdteMc/Zgpcx622+ljx7W+ztHno2r17cTTFJfIODwEuRfdTh87zvUd1HiJcR6H0Yuq1bcfDvRjpqx52LT/wPIYdVhEgxoJOegRlkVTtfr2yjn6MVPWPGxRpwo2KwO36MfMMOcEyTKMvN/pBeqfT7tTlDaVclF7N5gVkQP9mBlWXjItZ95YhizbdLOPZvXpbsF5WLN+ONCPoWHkJbPc9O00QKdTcG71+brmByQ7sBdHnpPEFWHCuSpwoLeWstz0TdumefrWtJZ+2ucFuT2P0/LV71XcoL7L2vPN2JLJazBVlpu+adtMSJmG4pf1xnKZ5TnNQtWmbBglB/oSGfTj9ZpvgqZtczxlEF5zC76I0xNYe3l27y1rV+EicqAvkTxbSFmGeKdtkzbpU3NLfdyGkVu+V2G+ohse5+hLJO8WUj+PiMvaBdQDXsZLnt17h9lVuOoc6Asgr54HRXlcnYeml1ee59a/k+HxpGYjNuxH8JlZOfnBIwU27Ly6mVWPUzcjNoq8uplVi1v0I+aeB2Y2aG7Rj1i7ngceHl5sPj82LhzoRyyt5wHg4eEF5uH7Nk7c66agVm16qGVXyZnpKc8DUwA+P1Y07nUzhjw8vNh8fmycOHUzIp3yu0UZ/GSt+fzYOHGLfgSyTD7mCb+KzefHxokD/QhkGSTlwU/F5vNj4yRT6kbSGuBWYAK4MyI2Na2/HPgq8EyyaEtEfCpZ9zHg31B7nsT/iIjP5lHwcZY1v+vBT8Xm82PjomOglzQB3A5cBcwDOyRti4gnmzZ9OCKuafrsO6gF+ZXAa8CDku6PiO/nUvqCS8vDO79rZsOUJXWzEjgQEU9HxGvAPcC1Gb//bcC3I+LViDgGfBP4l70Vdby0y8M7v2tmw5QldTMDHGx4Pw+8u8V2l0l6HDgE/PuI2As8AfwXSb8ELAJXAy07yEtaB6wDOO+88zJXoKjS8vB/+KXH+VkEvzg1yemTp3Dk1aNj94Bljwg1Gy9ZAr1aLGseZfUYcH5EvCzpamArsDwi9kn6r8A3gJeBx4FjrXYSEZuBzVAbMJWt+MWVloevP2bvyOJRpiYn+MwHL8kUJNOC67BHaHpEqNn4yZK6mQfObXh/DrVW+wkR8VJEvJy8fgCYlHRm8v4LEfHOiHgv8GOgEvn5LPn2rNMRt0sDDfsBy36gs9n4yRLodwDLJS2TdBqwFtjWuIGksyUpeb0y+d4XkvdnJX+fB1wP3J1f8YurVR6+lSwjKdsF12GP0PSIULPx0zF1ExHHJN0IbKfWvfKuiNgraX2y/g7gBuCjko5Ry8Wvjdcn0flykqM/Cvx+RPxkEBUpmubJyk6RTqRtGmVp+bcLrsPuweMeQ2bjJ1M/+iQd80DTsjsaXt8G3Jby2d/op4DjrLGfddpj/rL0tGkXXIf9gGU/0Nls/Hhk7JD0M5KyXXfMYY/Q9IhQs/HjaYrHhLs0mlk77aYp9uyVY8LD7c2sVw70OXBr28yKzIG+Tx5AZGZF55uxffIAIjMrOgf6PnkAkZkVnQN9n9IGCnkAkZkVhQN9nzzlsJkVnW/G9ql5qgP3ujGzonGgz4H7uJtZkTl1Y2ZWcg70ZmYl50BvZlZyDvRmZiXnQG9mVnIO9GZmJedAb2ZWcu5HnzNPWWxmReNAnyNPWWxmReRA30E3LfR2UxY70JvZqDjQt9FtC91TFptZEflmbBvdPlTEUxabWRE50LfRbQvdUxabWRE50LfRbQv9uktn+PT1FzEzPYWAmekpPn39Rc7Pm9lIOUffxobVK07K0UPnFrqnLDazonGgb8MPFTGzMsgU6CWtAW4FJoA7I2JT0/rLga8CzySLtkTEp5J1NwH/GghgD/C7EfH/8ij8MLiFbmbjrmOOXtIEcDvwPuDtwIckvb3Fpg9HxCXJn3qQnwH+AJiNiHdQ+49ibW6lNzOzjrK06FcCByLiaQBJ9wDXAk92sY8pSUeBM4BDvRS0CDy9gZmNoyy9bmaAgw3v55NlzS6T9Likr0m6ECAiFoA/AZ4Ffgi8GBFfb7UTSeskzUmaO3z4cFeVGIb64KmFI4sErw+e2rprYdRFMzNrK0ugV4tl0fT+MeD8iLgY+BywFUDSm6m1/pcBS4E3SfpIq51ExOaImI2I2SVLlmQsfu+27lpg1aaHWLbxflZteqhjwO528JSZWVFkCfTzwLkN78+hKf0SES9FxMvJ6weASUlnAr8FPBMRhyPiKLAF+PVcSt6HXlrnnt7AzMZVlkC/A1guaZmk06jdTN3WuIGksyUpeb0y+d4XqKVs3iPpjGT9lcC+PCvQi15a557ewMzGVcdAHxHHgBuB7dSC9JciYq+k9ZLWJ5vdADwh6XHgvwNro+ZR4D5qqZ09yf42D6AeXemlde7pDcxsXGXqR5+kYx5oWnZHw+vbgNtSPvtJ4JN9lDF3S6enWGgR1Nu1zj14yszGVaVGxta7Ry4cWUScfEc5S+vcg6fMbBxVJtA3zy0fcCLYz7h1bmYlVplA3+oGbD3IP7LxitEUysxsCCozTbG7R5pZVVUm0Lt7pJlVVWUCvbtHmllVVSZH7+6RZlZVlQn04O6RZlZNlQr0WXk6YjMrEwf6Js397esTngEO9mY2lipzMzYrT0dsZmXjQN/E/e3NrGwc6Ju4v72ZlY0DfRP3tzezsvHN2Cbub29mZeNA34L725tZmTh1Y2ZWcg70ZmYl50BvZlZyDvRmZiXnQG9mVnIO9GZmJedAb2ZWcg70ZmYl50BvZlZyDvRmZiWXKdBLWiNpv6QDkja2WH+5pBcl7U7+fCJZvqJh2W5JL0n6eM51MDOzNjrOdSNpArgduAqYB3ZI2hYRTzZt+nBEXNO4ICL2A5c0fM8C8JUcyp0rPzrQzMosS4t+JXAgIp6OiNeAe4Bre9jXlcDfR8QPevjswNQfHbhwZJHg9UcHbt21MOqimZnlIkugnwEONryfT5Y1u0zS45K+JunCFuvXAnf3UMaB8qMDzazssgR6tVgWTe8fA86PiIuBzwFbT/oC6TTgA8D/Tt2JtE7SnKS5w4cPZyhWPvzoQDMruyyBfh44t+H9OcChxg0i4qWIeDl5/QAwKenMhk3eBzwWET9K20lEbI6I2YiYXbJkSeYK9MuPDjSzsssS6HcAyyUtS1rma4FtjRtIOluSktcrk+99oWGTD1HAtA340YFmVn4de91ExDFJNwLbgQngrojYK2l9sv4O4Abgo5KOAYvA2ogIAElnUOux828HVIe++NGBZlZ2SuJxoczOzsbc3Nyoi2FmNjYk7YyI2VbrPDLWzKzkHOjNzErOgd7MrOQc6M3MSs6B3sys5BzozcxKzoHezKzkHOjNzErOgd7MrOQc6M3MSs6B3sys5BzozcxKzoHezKzkHOjNzErOgd7MrOQc6M3MSq7jE6bG3dZdC356lJlVWqkD/dZdC9y8ZQ+LR48DsHBkkZu37AFwsDezyih16uaW7ftPBPm6xaPHuWX7/hGVyMxs+Eod6A8dWexquZlZGZU60C+dnupquZlZGZU60G9YvYKpyYmTlk1NTrBh9YoRlcjMbPhKfTO2fsPVvW7MrMpKHeihFuwd2M2sykqdujEzMwd6M7PSc6A3Myu5TIFe0hpJ+yUdkLSxxfrLJb0oaXfy5xMN66Yl3SfpKUn7JF2WZwXMzKy9jjdjJU0AtwNXAfPADknbIuLJpk0fjohrWnzFrcCDEXGDpNOAM/ottJmZZZelRb8SOBART0fEa8A9wLVZvlzSLwDvBb4AEBGvRcSRHstqZmY9yBLoZ4CDDe/nk2XNLpP0uKSvSbowWfarwGHgzyTtknSnpDe12omkdZLmJM0dPny4mzqYmVkbWQK9WiyLpvePAedHxMXA54CtyfJTgXcCfxoRlwKvAG/I8QNExOaImI2I2SVLlmQpu5mZZZAl0M8D5za8Pwc41LhBRLwUES8nrx8AJiWdmXx2PiIeTTa9j1rgNzOzIckS6HcAyyUtS26mrgW2NW4g6WxJSl6vTL73hYj4R+CgpPrkMlcCzTdxzcxsgDr2uomIY5JuBLYDE8BdEbFX0vpk/R3ADcBHJR0DFoG1EVFP7/w74C+T/ySeBn53APUwM7MUej0eF8fs7GzMzc2NuhhmZmND0s6ImG21ziNjzcxKzoHezKzkHOjNzErOgd7MrORK+eCRrbsW/FQpM7NE6QL91l0L3LxlD4tHjwOwcGSRm7fsAXCwN7NKKl3q5pbt+08E+brFo8e5Zfv+EZXIzGy0ShfoDx1Z7Gq5mVnZlS7QL52e6mq5mVnZlS7Qb1i9gqnJiZOWTU1OsGH1ipRPmJmVW+luxtZvuLrXjZlZTekCPdSCvQO7mVlN6VI3ZmZ2Mgd6M7OSc6A3Mys5B3ozs5JzoDczKzkHejOzknOgNzMrudL0o/fUxGZmrZUi0HtqYjOzdKVI3XhqYjOzdKUI9J6a2MwsXSkCvacmNjNLV4pA76mJzczSleJmrKcmNjNLV4pAD56a2MwsTabUjaQ1kvZLOiBpY4v1l0t6UdLu5M8nGtb9g6Q9yfK5PAtvZmaddWzRS5oAbgeuAuaBHZK2RcSTTZs+HBHXpHzNb0bE8/0V1czMepGlRb8SOBART0fEa8A9wLWDLZaZmeUlS6CfAQ42vJ9PljW7TNLjkr4m6cKG5QF8XdJOSevSdiJpnaQ5SXOHDx/OVHgzM+ssy81YtVgWTe8fA86PiJclXQ1sBZYn61ZFxCFJZwHfkPRURHzrDV8YsRnYDDA7O9v8/WZm1qMsgX4eOLfh/TnAocYNIuKlhtcPSPq8pDMj4vmIOJQsf07SV6ilgt4Q6Bvt3LnzeUk/aFp8JlDVPL/rXk1VrXtV6w391f38tBVZAv0OYLmkZcACsBb4cOMGks4GfhQRIWkltZTQC5LeBJwSEf+UvP4XwKc67TAiljQvkzQXEbMZyls6rrvrXiVVrTcMru4dA31EHJN0I7AdmADuioi9ktYn6+8AbgA+KukYsAisTYL+LwNfkVTf119FxIN5V8LMzNJlGjAVEQ8ADzQtu6Ph9W3AbS0+9zRwcZ9lNDOzPozTXDebR12AEXLdq6mqda9qvWFAdVeEO7iYmZXZOLXozcysBw70ZmYlNxaBvtOkamUi6VxJ/0fSPkl7JX0sWf4WSd+Q9P3k7zePuqyDIGlC0i5Jf528r0q9pyXdJ+mp5NxfVqG635T81p+QdLek08tad0l3SXpO0hMNy1LrKunmJO7tl7S61/0WPtA3TKr2PuDtwIckvX20pRqoY8AfRsTbgPcAv5/UdyPwNxGxHPib5H0ZfQzY1/C+KvW+FXgwIn6NWk+1fVSg7pJmgD8AZiPiHdS6cK+lvHX/n8CapmUt65r8u18LXJh85vNJPOxa4QM9FZtULSJ+GBGPJa//ido/+Blqdf7zZLM/B64bSQEHSNI5wPuBOxsWV6HevwC8F/gCQES8FhFHqEDdE6cCU5JOBc6gNvK+lHVPpn/5cdPitLpeC9wTET+NiGeAA9TiYdfGIdBnnVStdCRdAFwKPAr8ckT8EGr/GQBnjbBog/JZ4D8AP2tYVoV6/ypwGPizJG11ZzKSvPR1j4gF4E+AZ4EfAi9GxNepQN0bpNU1t9g3DoE+y6RqpSPpnwFfBj7eOJdQWUm6BnguInaOuiwjcCrwTuBPI+JS4BXKk6poK8lHXwssA5YCb5L0kdGWqjByi33jEOg7TqpWNpImqQX5v4yILcniH0n6lWT9rwDPjap8A7IK+ICkf6CWnrtC0l9Q/npD7Tc+HxGPJu/voxb4q1D33wKeiYjDEXEU2AL8OtWoe11aXXOLfeMQ6E9MqibpNGo3J7aNuEwDo9rEQF8A9kXEf2tYtQ34neT17wBfHXbZBikibo6IcyLiAmrn+KGI+AglrzdARPwjcFDSimTRlcCTVKDu1FI275F0RvLbv5Lafakq1L0ura7bgLWSfi6ZVHI58J2e9hARhf8DXA18D/h74I9GXZ4B1/WfU7s8+y6wO/lzNfBL1O7Ifz/5+y2jLusAj8HlwF8nrytRb+ASYC4571uBN1eo7v8ZeAp4AvhfwM+Vte7A3dTuRRyl1mL/vXZ1Bf4oiXv7gff1ul9PgWBmVnLjkLoxM7M+ONCbmZWcA72ZWck50JuZlZwDvZlZyTnQm5mVnAO9mVnJ/X/Wv2eFAb7uMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(np.arange(3,101),s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot exemplifies how a growing forest initially tends to strongly improve the performance of the model, until it saturates at some constant maximum level, where the differences in the scores are only due to the noise that the randomness induces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
