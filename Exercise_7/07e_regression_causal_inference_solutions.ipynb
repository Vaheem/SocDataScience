{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Ridge Regression for Prediction and Causal Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from typing import List, Optional, Tuple, Dict, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Ridge Regression\n",
    "\n",
    "\n",
    "In lecture we have learned that in *ridge regression*, we want to minimize the loss function \n",
    "$$\n",
    "L(\\beta) = \\sum_{i=1}^n \\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2.\n",
    "$$\n",
    "\n",
    "#### a) Implementing the Loss\n",
    "\n",
    "Implement a function that computes the value of the loss function, using the signature in the cell below. Assume that the first column of your input matrix $X$ has all-constant values $1$ to model the intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X: np.ndarray, y: np.ndarray, beta: np.ndarray, reg: float) -> float:\n",
    "    \"\"\"\n",
    "    :param X: 2-dimensional numpy array of features \n",
    "    :param y: 1-dimensional array of target values\n",
    "    :param beta: current parameter vector\n",
    "    :param reg: float, regularization term (lambda)\n",
    "    :\n",
    "    :return: AUC score as float\n",
    "    \"\"\"\n",
    "    sse = np.sum(np.square(np.dot(X,beta) - y))\n",
    "    norm = np.sum(np.square(beta))\n",
    "    return sse + reg*norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Convexity of the Loss\n",
    "\n",
    "We explore the cost function with a practical example. Load the Iris dataset, and set up a univariate regression in which you predict petal width from petal length.\n",
    "Set  $\\beta_0 = mean(y)$ and $\\lambda = 1$, and plot the value of the cost function against $\\beta_1$ for  $\\beta_1 \\in\\{-10,-9.9,\\dots,9.9,10\\}$.  \n",
    "Is there a unique minimum? What is the sign of the derivative at $\\beta = -3$ and $\\beta = 3$? In which direction does it point - in direction of the minimum or against it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:,2]\n",
    "y = iris.data[:,3]\n",
    "\n",
    "X = np.vstack([np.ones(len(X)),X]).T\n",
    "\n",
    "b0 = np.mean(y)\n",
    "losses = []\n",
    "iv = np.arange(-10,10, step = 0.1)\n",
    "for b1 in iv:\n",
    "    losses.append(loss(X,y,np.array([b0,b1]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(iv,losses,s=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Gradient Descent\n",
    "\n",
    "In practice, regularized regression models regression are usually optimized by a variant of gradient descent. As discussed in lecture, in this iterative procedure one first initializes $\\beta$ at random, and then, until convergence, one updates\n",
    "$$\\beta^{new} \\gets \\beta^{old} - \\alpha\\cdot \\nabla L(\\beta^{old}), $$\n",
    "where $\\alpha$ is the so-called learning rate, and $\\nabla L(\\beta)$ the gradient of the loss function with respect to $\\beta$.\n",
    "As discussed in lecture, the gradient of the loss function is given by \n",
    "$$\n",
    "\\frac{\\partial C(\\beta)}{\\partial \\beta_k} = \\sum_{i=1}^n -2x_{ik} \\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right) + 2\\lambda\\beta_k,\n",
    "$$\n",
    "where for $\\beta_0$, we would have $x_{ik}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that optimizes a ridge regression model via gradient descent, using the function signature in the cell below. Initialize your model in a way that $\\beta_0$ equals the mean of $y$ and all other elements of $\\beta$ are drawn from the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_GD(X: np.ndarray, y: np.ndarray, reg: float, alpha: float, eps: float, max_iter: int=10000) -> Tuple[np.ndarray,int,np.ndarray]:\n",
    "    \"\"\"\n",
    "    :param X: 2-dimensional numpy array of features \n",
    "    :param y: 1-dimensional array of target values\n",
    "    :param reg: regularization term (lambda)\n",
    "    :param alpha: learning rate\n",
    "    :param eps: convergence treshold\n",
    "    :param max_iter: maximum number of iterations in gradient descent\n",
    "    : -> break iteration when that number is reached even is we have not yet converged\n",
    "    :\n",
    "    :return: Tuple of three variables:\n",
    "    :        1. 1D numpy array containing optimal coefficients beta\n",
    "    :        2. number of iterations n_it\n",
    "    :        3. vector of loss function values in every iteration\n",
    "    :\n",
    "    \"\"\"\n",
    "    n = (np.shape(X)[0])\n",
    "    p = np.shape(X)[1] # p is the number of variables in the multiple regression\n",
    "    \n",
    "    # initilize beta vector\n",
    "    b = np.random.normal(size=p+1)\n",
    "    b[0] = np.mean(y)\n",
    "    X = np.insert(X, 0, values=1, axis=1) # add a column of ones s.t. y=X*b\n",
    "\n",
    "    # initialize losses\n",
    "    losses = [loss(X,y,b,reg)]\n",
    "    n_it = 0\n",
    "    diff = eps+1\n",
    "    \n",
    "    # now the gradient descent:\n",
    "    while n_it < max_iter and diff > eps:\n",
    "        \n",
    "        b_old = np.copy(b)\n",
    "        res = (y - np.dot(X,b)) # residual from prediction with current beta\n",
    "        \n",
    "        # gradient computation can be written in a compact way\n",
    "        d_b = (-1)*np.dot(X.T,(2*res))+2*reg*b # derivative w.r.t. b\n",
    "        \n",
    "        # parameter update\n",
    "        b = b_old - alpha*d_b\n",
    "        losses.append(loss(X,y,b,reg))\n",
    "        diff = np.linalg.norm(losses[-1]-losses[-2])\n",
    "        \n",
    "        n_it+=1\n",
    "      \n",
    "    if n_it >=max_iter:\n",
    "        print(\"Iteration has not yet converged\")\n",
    "        \n",
    "    return b, n_it, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Regressing on Iris Data\n",
    "Rescale the numerical columns of the Iris data by the z-score and apply your implementation from c) to predict the petal width from all other columns, using $\\lambda = 1$, $\\alpha=0.001$, $\\epsilon=10^{-6}$. Compare the resulting parameters with those resulting from the ```statsmodels``` implementation of regularized regression fits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "X = iris.data[:,:-2]\n",
    "\n",
    "X = pd.DataFrame(X).apply(zscore)\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.19139121  0.59301713 -0.20704886]\n",
      "27\n",
      "[ 1.19139121  0.59301713 -0.20704886] 27 [1235.727199817765, 557.1892413103682, 258.73340821833926, 127.41463286718712, 69.612301250205, 44.15729205591773, 32.94082485633969, 27.99488344965184, 25.812061198186704, 24.84768995994245, 24.421088494329194, 24.232086332725487, 24.14819583874661, 24.110877752456638, 24.094233199094226, 24.086786132747523, 24.083441853057625, 24.081933517319612, 24.081249805637086, 24.080938094664692, 24.080795048853606, 24.080728920744356, 24.080698101749206, 24.080683611421875, 24.080676734005237, 24.080673437478943, 24.080671841266764, 24.08067106043177]\n"
     ]
    }
   ],
   "source": [
    "beta,n_it,losses = ridge_GD(X,y,reg=1,alpha=1e-03,eps=1e-06)\n",
    "print(beta)\n",
    "print(n_it)\n",
    "print(beta,n_it,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1981352 ,  0.59635388, -0.20782215])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#solution with statsmodels\n",
    "\n",
    "# need to add constant term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# initialize model: OLS = ordinary least squares\n",
    "model = sm.OLS(y,X)\n",
    "# fit model: only now the model, i.e. the parameters are computed\n",
    "results = model.fit_regularized(alpha=1e-03, L1_wt=0)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) The Effect of the Learning Rate\n",
    "\n",
    "Reoptimize your model using different learning rates $\\alpha \\in \\{10^{-i} : i\\in\\{1,2,3,4,5,6\\}\\}$.\n",
    "What are the effects on your optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vahee\\AppData\\Local\\Temp/ipykernel_21384/3399361051.py:10: RuntimeWarning: overflow encountered in square\n",
      "  sse = np.sum(np.square(np.dot(X,beta) - y))\n",
      "C:\\Users\\vahee\\AppData\\Local\\Temp/ipykernel_21384/3399361051.py:11: RuntimeWarning: overflow encountered in square\n",
      "  norm = np.sum(np.square(beta))\n",
      "C:\\Users\\vahee\\AppData\\Local\\Temp/ipykernel_21384/3847829045.py:42: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  diff = np.linalg.norm(losses[-1]-losses[-2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 88, array([ 3.68313915e+155,  3.68313915e+155, -2.67004164e+140,\n",
      "       -5.76981041e+140]), inf)\n",
      "(0.01, 221, array([-4.79944729e+153, -4.79944729e+153,  3.34369587e+138,\n",
      "        7.63471798e+138]), inf)\n",
      "(0.001, 1794, array([ 0.60882968,  0.58651916,  0.59308902, -0.2070336 ]), 23.368855402031596)\n",
      "Iteration has not yet converged\n",
      "(0.0001, 10000, array([ 0.65615325,  0.53919559,  0.59308902, -0.2070336 ]), 23.375446069250263)\n",
      "Iteration has not yet converged\n",
      "(1e-05, 10000, array([ 0.84537627,  0.34997257,  0.59308902, -0.2070336 ]), 23.491318934702754)\n",
      "Iteration has not yet converged\n",
      "(1e-06, 10000, array([ 1.94338017, -0.75178151,  0.42391033, -0.37567351]), 34.61257021465487)\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1e-01,1e-02,1e-03,1e-04,1e-05,1e-06]:\n",
    "    beta,n_it,losses = ridge_GD(X,y,reg=1,alpha=alpha,eps=1e-06)\n",
    "    print((alpha,n_it,beta,losses[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Causal Inference \n",
    "\n",
    "In this task we use a dataset (NSW.csv) which aimed to evaluate the effect of participating in a job training program on the salary. This data was taken from the website of Gelman and Hill's book (http://www.stat.columbia.edu/~gelman/arm/), and originally constructed in two independent studies (see Gelman and Hill, chapter 10, ex. 1).\n",
    "This data contains some demographic data of its population, the real earnings in 1974 and 1975, and indicator on whether job training, i.e., the treatment, was conducted in 1976/77, and the earnings in 1978, which is our target variable. A brief documentation can be found in \"NSW.doc\". Make sure that when loading the data, you omit the sample variable which simply indicates a source that a specific observation originated from.  \n",
    "Note that there are only very few treated individuals in the dataset.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>hisp</th>\n",
       "      <th>sample</th>\n",
       "      <th>treat</th>\n",
       "      <th>educ_cat4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.485405</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2366.794189</td>\n",
       "      <td>3317.467773</td>\n",
       "      <td>4793.745117</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25862.322266</td>\n",
       "      <td>22781.855469</td>\n",
       "      <td>25564.669922</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21591.121094</td>\n",
       "      <td>20839.355469</td>\n",
       "      <td>20550.744141</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21395.193359</td>\n",
       "      <td>21575.177734</td>\n",
       "      <td>22783.587891</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  educ  black  married  nodegree          re74          re75  \\\n",
       "1   42    16      0        1         0      0.000000      0.000000   \n",
       "2   20    13      0        0         0   2366.794189   3317.467773   \n",
       "3   37    12      0        1         0  25862.322266  22781.855469   \n",
       "4   48    12      0        1         0  21591.121094  20839.355469   \n",
       "5   51    12      0        1         0  21395.193359  21575.177734   \n",
       "\n",
       "           re78  hisp  sample  treat  educ_cat4  \n",
       "1    100.485405     0       2      0          4  \n",
       "2   4793.745117     0       2      0          3  \n",
       "3  25564.669922     0       2      0          2  \n",
       "4  20550.744141     0       2      0          2  \n",
       "5  22783.587891     0       2      0          2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta = pd.read_csv(\"NSW.csv\", index_col=0)\n",
    "dta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Implementing Stratification\n",
    "\n",
    "Write a function that applies stratification for causal inference and computes the corresponding average treatment effect. \n",
    "Make sure to introduce strata of (nearly) equal size, and apply a logistic regression classifier to model the propensity scores. Use the function signature specified in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify(X: np.ndarray, y: np.ndarray, T: np.ndarray, n: int) -> float:\n",
    "    \"\"\"\n",
    "    :param X: 2-dimensional numpy array of covariates, excluding treatment and target variable \n",
    "    :param T: 1-dimensional array of binary treatment values, corresponding to instances of X\n",
    "    :param y: 1-dimensional array of target values\n",
    "    :param n: number of strata to split the data into\n",
    "    :\n",
    "    :return: average treatment effect as float\n",
    "    \"\"\"    \n",
    "    nrow = len(y)\n",
    "    \n",
    "    # compute propensity scores\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    clf.fit(X,T)\n",
    "    ps = clf.predict_proba(X)[:,1]\n",
    "    \n",
    "    # assign strata\n",
    "    strata = np.ceil((ps.argsort()+1) / nrow*n)\n",
    "    \n",
    "    # helper variables\n",
    "    T_y = T * y\n",
    "    Tbar_y = (1-T) * y\n",
    "    \n",
    "    # iterate over strata\n",
    "    res = 0.0\n",
    "    for s in range(1,n+1):\n",
    "        idx = np.where(strata == s)[0]\n",
    "        n_tr = np.sum(T[idx])\n",
    "        n_ct = len(idx) - n_tr\n",
    "        res += 1.0/n_tr*(np.sum(T_y[idx])/n_tr - np.sum(Tbar_y[idx])/n_ct)\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Applying Stratification\n",
    "\n",
    "Apply your implementation from a) on the NSW data, using $n=5$ strata. Feel free to apply varying numbers of strata. Does a big number like $n>100$ make sense for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = ['age','black','hisp','married','educ','nodegree','re74','re75']\n",
    "X = dta[pred_cols].to_numpy()\n",
    "T = dta.treat.to_numpy()\n",
    "y = dta.re78.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1786.286462003439"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratify(X, y, T, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ Due to only very few treated in the samples, large numbers of strata are a bad idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Implementing Propensity Score Weighting\n",
    "\n",
    "Write a function that applies propensity score weighting for causal inference and computes the corresponding average treatment effect. Again, apply a logistic regression classifier to model the propensity scores. Use the function signature specified in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity_weighting(X: np.ndarray, y: np.ndarray, T: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    :param X: 2-dimensional numpy array of covariates, excluding treatment and target variable \n",
    "    :param T: 1-dimensional array of binary treatment values, corresponding to instances of X\n",
    "    :param y: 1-dimensional array of target values\n",
    "    :\n",
    "    :return: average treatment effect as float\n",
    "    \"\"\"    \n",
    "    nrow = len(y)\n",
    "    \n",
    "    # compute propensity scores\n",
    "    clf = LogisticRegression(max_iter = 10000)\n",
    "    clf.fit(X,T)\n",
    "    ps = clf.predict_proba(X)[:,1]\n",
    "    \n",
    "    # declare helper variables\n",
    "    w = np.zeros(nrow)\n",
    "    N_tr, N_ct = 0,0\n",
    "    sum_tr, sum_ct = 0,0\n",
    "    \n",
    "    # iterate over all samples\n",
    "    for i in range(nrow):\n",
    "        \n",
    "        # compute weight w_i\n",
    "        w[i] = (T[i])/(ps[i]) + (1-T[i])/(1-ps[i])\n",
    "        \n",
    "        # update sums elementwise\n",
    "        N_tr += T[i]/ps[i]\n",
    "        N_ct += (1-T[i])/(1-ps[i])\n",
    "        \n",
    "        sum_tr += T[i]*w[i]*y[i]\n",
    "        sum_ct += (1-T[i])*w[i]*y[i]\n",
    "        \n",
    "    return 1/N_tr * sum_tr - 1/N_ct * sum_ct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Applying Propensity Score Weighting\n",
    "\n",
    "Apply your implementation from c) on the NSW data, and compare the result to the ATE from b) as well as the unweighted and unstratified ATE. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8309.539404883646"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity_weighting(X, y, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: Again, we seem to observe a very negative treatment effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Propensity Score Matching\n",
    "\n",
    "Finally, we want to apply propensity score matching on the NSW data.\n",
    "Note that due to the strong imbalance between the cardinalities of the control group and the treatment group, you do not need to consider a calliper in this task, and may implicitly assume that the treated group is much smaller in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity_matching(X: np.ndarray, y: np.ndarray, T: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    :param X: 2-dimensional numpy array of covariates, excluding treatment and target variable \n",
    "    :param T: 1-dimensional array of binary treatment values, corresponding to instances of X\n",
    "    :param y: 1-dimensional array of target values\n",
    "    :\n",
    "    :return: average treatment effect as float\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute propensity scores\n",
    "    clf = LogisticRegression(max_iter = 10000)\n",
    "    clf.fit(X,T)\n",
    "    ps = clf.predict_proba(X)[:,1]\n",
    "    \n",
    "    y_treated = y[T==1]\n",
    "    y_untreated = y[T==0]\n",
    "    ps_treated = ps[T==1]\n",
    "    ps_untreated = ps[T==0]\n",
    "    \n",
    "    idx_matches = []\n",
    "    best_match=0\n",
    "    for i,o2 in enumerate(ps_treated):\n",
    "        \n",
    "        #compute distances:\n",
    "        distances = [abs(o1 - o2) for o1 in ps_untreated]\n",
    "        \n",
    "        # get best match via argmin\n",
    "        best_match = np.argmin(distances)\n",
    "        \n",
    "        #store the matching\n",
    "        idx_matches.append(best_match)\n",
    "        \n",
    "        # make sure that matched instances are not re-assigned\n",
    "        ps_untreated[best_match] = np.inf\n",
    "        \n",
    "    return np.mean(y_treated - y_untreated[idx_matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Applying Greedy Matching\n",
    "\n",
    "Apply your implementation from e) on the NSW data and use the matching to compute the ATE score on matched data, and compare the result to the ATEs from b) and d) as well as the unweighted and unstratified ATE. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803.4433216610468"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity_matching(X, y, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: According to propensity score matching, job training now seems to have a very positive effect on the income."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
